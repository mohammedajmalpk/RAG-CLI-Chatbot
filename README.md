ğŸ“˜ RAG-CLI-Chatbot

A CLI-based Retrieval-Augmented Generation (RAG) chatbot that reads PDFs, creates embeddings using HuggingFace Sentence-Transformers, stores vectors in FAISS, and answers user questions using semantic search. Ollama LLM (llama3.2) is used as the transformer for generation.

 - ğŸš€ Features 
 - ğŸ“„ Extract text from PDF files 
 - âœ‚ï¸ Smart text chunking 
 - ğŸ” Create embeddings using all-MiniLM-L6-v2 (HuggingFace)
 - ğŸ§  Store vectors in FAISS (CPU-friendly)
 - â“ Ask queries and retrieve top matching chunks 
 - ğŸ¤– Generate answers using Ollama LLM (llama3.2)
 - ğŸ’» Fully CLI-based (no GUI required)

âš¡ How It Works

 - Load PDFs â†’ extract raw text. 
 - Chunk text â†’ divide into smaller, meaningful pieces. 
 - Generate embeddings â†’ convert chunks into vector representations. 
 - Store in FAISS â†’ fast vector search. 
 - Query system â†’ retrieve top matching chunks using semantic search. 
 - Answer generation â†’ Ollama LLM generates context-aware answers.

ğŸ› ï¸ Requirements

 - Python 3.11+ 
 - Ollama - installed with llama3.2 model (locally run)
 - HuggingFace sentence-transformers 
 - FAISS (faiss-cpu)
 - PyPDF2 (for PDF text extraction)